## 模型构造
继承nn.Module：比Sequential更灵活，也可以和Sequential互相嵌套
FixedHiddenMLP()


## 参数管理
`net[2].state_dict() #返回权重和偏置的OrderedDict
`net[2].bias.data    
`net[2].weight.grad

net.add

带_是替换函数，原地改变
`net.apply( )  #full loop
`net.constant_
`nn.init.xavier_uniform_(m.weight)

#### 参数绑定
shared，多次进Sequential
## 自定义层
和自定义网络没有本质区别

- 带参数、ReLU的

## 读写文件
`torch.save(obj, 'addr'), torch.load('addr')
难以存下整个网络
`torch.save(net.state_dict(), 'addr')
`net_clone.load_state_dict(torch.load(' '))

## QA
伪变量炸内存：
    稀疏矩阵
    不用one-hot
`__call__()
