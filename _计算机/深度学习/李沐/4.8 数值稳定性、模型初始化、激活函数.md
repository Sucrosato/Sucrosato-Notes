梯度爆炸（ReLU为例）
梯度消失（sigmoid为例）：底部层训练不动
深度模型中更常见



## 让训练更加稳定
乘法 -> 加法 ： ResNet，LSTM
归一化
合理的权重初始和激活函数

## 方差常数：
对于任意一层，期望和方差不变：
$$E = 0,var=b$$
#### 权重

正向推导：
$$n_{t-1}\gamma_{t} = 1$$
反向：
$$n_{t}\gamma_{t}=1$$
难以同时满足，Xavier初始：
$$\frac{\gamma_{t}(n_{t-1} + n_{t})}{2} = 1$$
$\gamma_{t}$ 是方差
对于正态分布和均匀分布，选择合适的初始方差进行权重初始化
#### 激活函数：
如果是线性，则必须斜率平方为1，截距为0
如果是sigmoid，则泰勒展开后发现要做调整：
$$\mathrm{sigmoid'} = 4\times \mathrm{sigmoid} -2$$
