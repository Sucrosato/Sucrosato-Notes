二分类
等价于$l=max(0, y(\vec{w}\cdot \vec{x}))$、批量大小为1的梯度下降
收敛定理

#### 单隐藏层：
大小是超参数

为什么需要非线性的激活函数$\sigma()$：塌陷成线性

#### 激活函数
##### Sigmoid
$$\mathrm{sigmoid}(x) = \frac{1}{1+e^{-x}} \in[0, 1]$$
很贵，$\approx$ 100次乘法
##### Tanh
:(-1, 1)
##### ReLU *Rectified Linear Unit*
$$\mathrm{ReLU}(x) = max(x, 0)$$

#### 多隐藏层
超参数：
    层数
    每层大小
直觉：
    第一层大一点
    慢慢压缩
输出层还要加softmax，但是不用激活（只是为了防止塌陷）