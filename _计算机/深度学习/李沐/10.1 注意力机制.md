bmm：batch mat mal, (b, l, m) @ (b, m, n) = (b, l, n)

# 10.3 注意力分数
要扩展到qkv都是向量，分数可以选择：
#### Additive Attention
可加：
$$a(\mathbf{k}, \mathbf{q})= \mathbf{v}^T\mathrm{\tanh}(\mathbf{W}_{k}\mathbf{k}+\mathbf{W}_{q}\mathbf{q})$$
允许不同长度的key和query
##### 实现：
三个线性层，bias=False
dropout
unsqueeze，广播实现每个query和key相加，(batch_size, num_queries, num_keys, num_hiddens)
masked_softmax
bmm: 批量相乘
#### Scaled Dot-Product Attention
如果有相同长度d：
$$a(\mathbf{q, k_{i}}) = \frac{\langle\mathbf{q, k_{i}}\rangle}{\sqrt{ d }}$$
实现简单，超参数只有d
#### mask-softmax
为了把pad（不够长时的填充）过滤掉

# 使用注意力机制的seq2seq
