bmm：batch mat mal, (b, l, m) @ (b, m, n) = (b, l, n)

# 10.3 注意力分数
要扩展到qkv都是向量，分数可以选择：
#### Additive Attention
可加：
$$a(\mathbf{k}, \mathbf{q})= \mathbf{v}^T\mathrm{\tanh}(\mathbf{W}_{k}\mathbf{k}+\mathbf{W}_{q}\mathbf{q})$$
允许不同长度的key和query
#### Scaled Dot-Product Attention
如果有相同长度d：
$$a(\mathbf{q, k_{i}}) = \frac{\langle\mathbf{q, k_{i}}\rangle}{\sqrt{ d }}$$
